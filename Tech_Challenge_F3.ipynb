{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiTxVzipoBR6"
   },
   "source": [
    "# Preparando os dados para o [fine-tuning]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Nzpjk36_9hd"
   },
   "outputs": [],
   "source": [
    "# Montando Google-Drive:\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mVwY5HRdA83o"
   },
   "outputs": [],
   "source": [
    "# Descompactando LF-Amazon:\n",
    "\n",
    "#!unzip '/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M.raw.zip' -d '/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JXlp_0pCHQk"
   },
   "outputs": [],
   "source": [
    "# Descompactando arquivos json:\n",
    "\n",
    "#!gunzip '/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/lbl.json.gz'\n",
    "#!gunzip '/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/trn.json.gz'\n",
    "#!gunzip '/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/tst.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7HZ4ecpH_nI"
   },
   "outputs": [],
   "source": [
    "# Verificando as cinco primeiras linhas do arquivo [trn.json]:\n",
    "\n",
    "# Defina o caminho para o arquivo de treino que você descompactou\n",
    "caminho_arquivo_treino = r'/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/trn.json'\n",
    "\n",
    "# Vamos ler e imprimir apenas as 5 primeiras linhas\n",
    "print(f\"--- Analisando as primeiras 5 linhas de: {caminho_arquivo_treino} ---\")\n",
    "try:\n",
    "    with open(caminho_arquivo_treino, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            print(f\"Linha {i+1}: {line.strip()}\")\n",
    "            if i >= 4:  # Parar depois de 5 linhas (índice 0 a 4)\n",
    "                break\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao ler o arquivo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVkBpYpwMFQ6"
   },
   "outputs": [],
   "source": [
    "# Verificando as cinco primeiras linhas do arquivo [tst.json]:\n",
    "\n",
    "# Defina o caminho para o arquivo de treino que você descompactou\n",
    "caminho_arquivo_treino = r'/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/tst.json'\n",
    "\n",
    "# Vamos ler e imprimir apenas as 5 primeiras linhas\n",
    "print(f\"--- Analisando as primeiras 5 linhas de: {caminho_arquivo_treino} ---\")\n",
    "try:\n",
    "    with open(caminho_arquivo_treino, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            print(f\"Linha {i+1}: {line.strip()}\")\n",
    "            if i >= 4:  # Parar depois de 5 linhas (índice 0 a 4)\n",
    "                break\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao ler o arquivo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7T0vTL_xOW6H"
   },
   "outputs": [],
   "source": [
    "# Verificando as cinco primeiras linhas do arquivo [lbl.json]:\n",
    "\n",
    "# Defina o caminho para o arquivo de treino que você descompactou\n",
    "caminho_arquivo_treino = r'/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/lbl.json'\n",
    "\n",
    "# Vamos ler e imprimir apenas as 5 primeiras linhas\n",
    "print(f\"--- Analisando as primeiras 5 linhas de: {caminho_arquivo_treino} ---\")\n",
    "try:\n",
    "    with open(caminho_arquivo_treino, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            print(f\"Linha {i+1}: {line.strip()}\")\n",
    "            if i >= 4:  # Parar depois de 5 linhas (índice 0 a 4)\n",
    "                break\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao ler o arquivo: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vpS9EJmQQ2G"
   },
   "source": [
    "# Script de Processamento\n",
    "> Segundo as instruções do Tech Challenge, serão utilizadas somente duas coluas:\n",
    "```\n",
    "...\n",
    "você utilizará as colunas “title” e\n",
    "“content”, que contém título e descrição respectivamente.\n",
    "...\n",
    "(Tech Challenge.pdf)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBXmK4_cOzSw"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import html # Estou usando esta lib para tratar as [Entidades HTML]; quero que o texto seja apresentado corretamente.\n",
    "import re # Importe a biblioteca de expressões regulares para limpeza extra.\n",
    "\n",
    "# --- CONFIGURAÇÃO ---\n",
    "# Caminhos para o Googl-Drive.\n",
    "DRIVE_BASE_PATH = r\"/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M\"\n",
    "INPUT_FILE_PATH = os.path.join(DRIVE_BASE_PATH, 'trn.json')\n",
    "OUTPUT_FILE_PATH = os.path.join(DRIVE_BASE_PATH, 'dataset_para_finetuning.jsonl')\n",
    "\n",
    "# Defina quantos exemplos você quer processar.\n",
    "# Comece com um número menor (ex: 20000) para testar o fluxo.\n",
    "# Para o treino final, você pode aumentar se necessário.\n",
    "MAX_EXAMPLES = 50000\n",
    "count = 0\n",
    "\n",
    "# Template de prompt nativo do Llama-3\n",
    "LLAMA3_PROMPT_TEMPLATE = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Com base no título do produto, gere a sua descrição.\n",
    "Título: {}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|><|end_of_text|>\"\"\"\n",
    "\n",
    "print(\"Iniciando o processamento do dataset para o formato nativo Llama-3, com limpeza de HTML...\")\n",
    "\n",
    "# Usamos 'with' para garantir que os arquivos sejam fechados corretamente\n",
    "with open(INPUT_FILE_PATH, 'r', encoding='utf-8') as infile, \\\n",
    "     open(OUTPUT_FILE_PATH, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "    for line in infile:\n",
    "        # Para o loop quando atingir o número desejado de exemplos\n",
    "        if count >= MAX_EXAMPLES:\n",
    "            print(f\"Limite de {MAX_EXAMPLES} exemplos atingido. Parando o processamento.\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # Carrega a linha atual (que é uma string JSON) para um dicionário Python\n",
    "            original_record = json.loads(line)\n",
    "\n",
    "            # Extrai os campos que nos interessam usando .get() para evitar erros\n",
    "            title = html.unescape(original_record.get('title', ''))\n",
    "            content = html.unescape(original_record.get('content', ''))\n",
    "\n",
    "            # --- Passo de Limpeza e Validação Crucial ---\n",
    "            # Pular registros que não têm título ou que têm conteúdo vazio\n",
    "            if not title or not content:\n",
    "                continue\n",
    "\n",
    "            # --- ETAPA DE LIMPEZA ADICIONAL ---\n",
    "            # Remove a frase \"--This text refers to...\" e qualquer variação dela.\n",
    "            content = re.sub(r'--This text refers to.*', '', content).strip()\n",
    "\n",
    "            # Se após a limpeza o conteúdo ficar vazio, pule o registro.\n",
    "            if not content:\n",
    "                continue\n",
    "            # --- FIM DA LIMPEZA ---\n",
    "\n",
    "            # Cria o novo dicionário no formato de instrução que o modelo espera\n",
    "            # NOTE: A instrução está em português para clareza, enquanto os dados\n",
    "            # de input/output estão em inglês, aproveitando a capacidade\n",
    "            # multilingue do modelo.\n",
    "            # Usamos um único campo \"text\" que já contém o prompt completo e formatado\n",
    "            formatted_text = LLAMA3_PROMPT_TEMPLATE.format(title, content)\n",
    "\n",
    "            # Escrevemos um dicionário contendo apenas a chave \"text\"\n",
    "            new_record = {\"text\": formatted_text}\n",
    "\n",
    "            outfile.write(json.dumps(new_record, ensure_ascii=False) + '\\n')\n",
    "            count += 1\n",
    "\n",
    "            # Imprime um status a cada 1000 registros processados\n",
    "            if count % 5000 == 0:\n",
    "                print(f\"Processados {count} registros...\")\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            # Ignora linhas que não sejam um JSON válido\n",
    "            continue\n",
    "\n",
    "print(f\"\\nProcessamento concluído! {count} exemplos válidos foram salvos em '{OUTPUT_FILE_PATH}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SaKRpGFyOjzt"
   },
   "outputs": [],
   "source": [
    "# Verificando o conteúdo do arquivo gerado:\n",
    "! head '/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/dataset_para_finetuning.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzBVlUFIoOX5"
   },
   "source": [
    "## Conclusão da preparação dos dados\n",
    "> Foram utilizado 50.000 registros para amostragem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76THYRp_pAOB"
   },
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhAHQVN_dyCY"
   },
   "outputs": [],
   "source": [
    "# Montando Google-Drive:\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kS_CJACpjh5"
   },
   "outputs": [],
   "source": [
    "# Instalando libs necessárias:\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83kXRem0WKJN"
   },
   "outputs": [],
   "source": [
    "# Configuração\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "DATA_PATH = r'/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/dataset_para_finetuning.jsonl'\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",\n",
    "]\n",
    "\n",
    "# Carrega o dataset a partir do arquivo JSON formatado\n",
    "dataset = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XP63HrhRsO9R"
   },
   "outputs": [],
   "source": [
    "# Instanciando o Modelo:\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = fourbit_models[2],\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QP4UVbhsXqg"
   },
   "outputs": [],
   "source": [
    "# Preparando o Modelo LoRA:\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNHMMMh4tKp5"
   },
   "outputs": [],
   "source": [
    "# Ajustes finais para treinar o modelo\n",
    "\n",
    "# O diretório no seu Drive para salvar o modelo final e os checkpoints\n",
    "output_dir = r\"/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/FineTuning_Outputs\"\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model = model,\n",
    "  tokenizer = tokenizer,\n",
    "  train_dataset = dataset,\n",
    "  dataset_text_field = \"text\",\n",
    "  max_seq_length = max_seq_length,\n",
    "  dataset_num_proc = 2,\n",
    "  packing = False,\n",
    "  args = TrainingArguments(\n",
    "      # --- Parâmetros de Desempenho ---\n",
    "      per_device_train_batch_size = 2,\n",
    "      gradient_accumulation_steps = 4,\n",
    "      warmup_steps = 5,\n",
    "      # max_steps = 60, # Apenas para testes.\n",
    "      num_train_epochs = 2, # Usando todo o dataset duas vezes.\n",
    "      learning_rate = 2e-4, # Força a prestar muito mais atenção aos detalhes de cada exemplo.\n",
    "      fp16 = not is_bfloat16_supported(),\n",
    "      bf16 = is_bfloat16_supported(),\n",
    "      optim = \"adamw_8bit\",\n",
    "      weight_decay = 0.01,\n",
    "      lr_scheduler_type = \"linear\",\n",
    "      seed = 3407,\n",
    "\n",
    "      # --- Parâmetros de LOGGING e CHECKPOINT ---\n",
    "      output_dir = output_dir,          # Diretório para salvar tudo\n",
    "      logging_steps = 1,                 # Mostra a 'loss' a cada passo\n",
    "      report_to = \"none\",              # Desabilita o login do wandb\n",
    "      save_strategy = \"steps\",         # Estratégia para salvar: a cada X passos\n",
    "      save_steps = 100,                 # Salva um checkpoint a cada 100 passos\n",
    "      save_total_limit = 2,            # Mantém apenas os 2 últimos checkpoints para não encher seu Drive\n",
    "  ),\n",
    ")\n",
    "\n",
    "# Para iniciar o treinamento, use:\n",
    "# trainer.train() # <--- Use esta linha se for a PRIMEIRA vez que está treinando\n",
    "# trainer.train(resume_from_checkpoint = True) # <--- Use esta se precisar CONTINUAR um treino interrompido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPVwuciVtODB"
   },
   "outputs": [],
   "source": [
    "# Fazendo o treinamento\n",
    "# E para iniciar o treinamento, use:\n",
    "# trainer.train() # <--- Use esta linha se for a PRIMEIRA vez que está treinando\n",
    "# trainer.train(resume_from_checkpoint = True) # <--- Use esta se precisar CONTINUAR um treino interrompido\n",
    "\n",
    "# trainer_stats = trainer.train()\n",
    "trainer_stats = trainer.train(resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b4y7xG2NctLp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZA_XyU63TZcF"
   },
   "source": [
    "# Testando o Modelo Fine-Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mi5qPuG0QJzM"
   },
   "outputs": [],
   "source": [
    "# Montar o Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Instalar as bibliotecas necessárias.\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_XHgj1Yg3dF"
   },
   "outputs": [],
   "source": [
    "# Carregando o Modelo Fine-Tuned (Método de 2 Etapas)\n",
    "\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# --- Configuração ---\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "# --- CAMINHO PARA O ADAPTADOR TREINADO ---\n",
    "# Este é o caminho para os pesos do fine-tuning\n",
    "caminho_do_adaptador_lora = r\"/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/FineTuning_Outputs/checkpoint-12500\"\n",
    "\n",
    "# --- NOME DO MODELO BASE ORIGINAL ---\n",
    "# Este é o modelo que foi usado para o treinamento inicial\n",
    "nome_do_modelo_base = \"unsloth/llama-3-8b-bnb-4bit\"\n",
    "\n",
    "\n",
    "# ETAPA 1: Carregue o modelo BASE original a partir da internet.\n",
    "# Isso garante que a Unsloth tenha o 'config.json' correto e saiba que está lidando com um Llama 3.\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = nome_do_modelo_base,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "# ETAPA 2: Aplicando o adaptador LoRA treinado (pesos) por cima do modelo base.\n",
    "model.load_adapter(caminho_do_adaptador_lora)\n",
    "\n",
    "\n",
    "# Prepara o modelo final para inferência (mais rápido)\n",
    "FastLanguageModel.for_inference(model)\n",
    "print(\"Modelo carregado com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2yZGlQZoTzT9"
   },
   "outputs": [],
   "source": [
    "# Loop de Teste Interativo (VERSÃO CORRIGIDA PARA LLAMA-3)\n",
    "\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "\n",
    "# Define o template do prompt para INFERÊNCIA.\n",
    "# Note que ele termina exatamente onde a resposta do assistente deve começar.\n",
    "LLAMA3_INFERENCE_PROMPT = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Com base no título do produto, gere a sua descrição.\n",
    "Título: {}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Loop para testar o modelo várias vezes\n",
    "while True:\n",
    "    # Pede um título de produto para o usuário\n",
    "    titulo_produto = input(\"Digite o título do produto (ou 'sair' para terminar): \")\n",
    "\n",
    "    # Condição para sair do loop\n",
    "    if titulo_produto.lower() == 'sair':\n",
    "        print(\"Encerrando o teste.\")\n",
    "        break\n",
    "\n",
    "    # Formata o prompt para o modelo usando o novo template Llama-3\n",
    "    prompt = LLAMA3_INFERENCE_PROMPT.format(titulo_produto)\n",
    "\n",
    "    # Tokeniza o prompt e o envia para a GPU\n",
    "    inputs = tokenizer([prompt], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    # Usa o TextStreamer para ver a resposta sendo gerada em tempo real\n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt=True) # skip_prompt=True é útil aqui\n",
    "    _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n",
    "                       eos_token_id=tokenizer.eos_token_id,\n",
    "                       repetition_penalty=1.15) # Adicionado para parar corretamente\n",
    "\n",
    "    # Adiciona uma linha de separação para o próximo teste\n",
    "    print(\"\\\\n\" + \"=\"*50 + \"\\\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Para Hugging Face Hub"
   ],
   "metadata": {
    "id": "MCOUNUh9MlBK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Montar o Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "5HS17t7IMyJH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Instalar as bibliotecas necessárias.\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ],
   "metadata": {
    "id": "2GfxnHflPjP0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# SALVANDO O MODELO FINAL\n",
    "\n",
    "\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# --- Caminho onde o checkpoint final foi salvo ---\n",
    "caminho_do_checkpoint_final = r\"/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/FineTuning_Outputs/checkpoint-12500\"\n",
    "\n",
    "# --- Onde salvar o modelo pronto para upload ---\n",
    "caminho_para_salvar_modelo = r\"/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/modelo_final_Llama3_8b_TCF3\"\n",
    "\n",
    "# Carrega o modelo a partir do checkpoint final\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = caminho_do_checkpoint_final,\n",
    ")\n",
    "\n",
    "# Salva o modelo e o tokenizador em um novo diretório.\n",
    "# Este comando junta o adaptador LoRA com os arquivos de configuração necessários.\n",
    "model.save_pretrained(caminho_para_salvar_modelo)\n",
    "tokenizer.save_pretrained(caminho_para_salvar_modelo)\n",
    "\n",
    "print(f\"Modelo final salvo com sucesso em: {caminho_para_salvar_modelo}\")"
   ],
   "metadata": {
    "id": "AZnMPlh7NDTC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# UPLOAD PARA O HUGGING FACE HUB\n",
    "\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# token de acesso do Hugging\n",
    "login(\"\")\n",
    "\n",
    "# --- Onde o modelo final foi salvo ---\n",
    "caminho_do_modelo_salvo = r\"/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/modelo_final_Llama3_8b_TCF3\"\n",
    "\n",
    "# --- Nome no repositório no Hugging Face ---\n",
    "nome_do_repo_hf = \"robsonnicacio/llama-3-8b-amazon-descriptions-tcf3\"\n",
    "\n",
    "# Carrega o tokenizador e o modelo localmente\n",
    "tokenizer = AutoTokenizer.from_pretrained(caminho_do_modelo_salvo)\n",
    "model = AutoModelForCausalLM.from_pretrained(caminho_do_modelo_salvo)\n",
    "\n",
    "# Faz o upload\n",
    "tokenizer.push_to_hub(nome_do_repo_hf, use_temp_dir=True)\n",
    "model.push_to_hub(nome_do_repo_hf, use_temp_dir=True)\n",
    "\n",
    "print(f\"Upload concluído! Seu modelo está disponível em: https://huggingface.co/{nome_do_repo_hf}\")"
   ],
   "metadata": {
    "id": "1mcuwJ8wNJQq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Usando o Modelo Treinado via Hugging Face Hub"
   ],
   "metadata": {
    "id": "u_y60TZWNfCa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Instalando as bibliotecas necessárias.\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ],
   "metadata": {
    "id": "gX7sDXwPwRkc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Carregando o Modelo Fine-Tuned DIRETAMENTE DO HUGGING FACE\n",
    "# OBS: falta a parte para inferência aqui!\n",
    "\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# --- NOME DO SEU MODELO NO HUGGING FACE ---\n",
    "nome_do_seu_modelo_no_hf = \"robsonnicacio/llama-3-8b-amazon-descriptions-tcf3\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = nome_do_seu_modelo_no_hf,\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# Prepara o modelo final para inferência\n",
    "FastLanguageModel.for_inference(model)\n",
    "print(\"Modelo fine-tuned carregado com sucesso do Hugging Face Hub!\")"
   ],
   "metadata": {
    "id": "iVPbSUJHNqjp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Loop de Teste Interativo (Testando o que está no Hugging Face)\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "\n",
    "# Define o template do prompt para INFERÊNCIA.\n",
    "# Note que ele termina exatamente onde a resposta do assistente deve começar.\n",
    "LLAMA3_INFERENCE_PROMPT = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Com base no título do produto, gere a sua descrição.\n",
    "Título: {}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Loop para testar o modelo várias vezes\n",
    "while True:\n",
    "    # Pede um título de produto para o usuário\n",
    "    titulo_produto = input(\"Digite o título do produto (ou 'sair' para terminar): \")\n",
    "\n",
    "    # Condição para sair do loop\n",
    "    if titulo_produto.lower() == 'sair':\n",
    "        print(\"Encerrando o teste.\")\n",
    "        break\n",
    "\n",
    "    # Formata o prompt para o modelo usando o novo template Llama-3\n",
    "    prompt = LLAMA3_INFERENCE_PROMPT.format(titulo_produto)\n",
    "\n",
    "    # Tokeniza o prompt e o envia para a GPU\n",
    "    inputs = tokenizer([prompt], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    # Usa o TextStreamer para ver a resposta sendo gerada em tempo real\n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt=True) # skip_prompt=True é útil aqui\n",
    "    _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n",
    "                       eos_token_id=tokenizer.eos_token_id,\n",
    "                       repetition_penalty = 1.15) # Adicionado para parar corretamente\n",
    "\n",
    "    # Adiciona uma linha de separação para o próximo teste\n",
    "    print(\"\\\\n\" + \"=\"*50 + \"\\\\n\")"
   ],
   "metadata": {
    "id": "keua1fHaNwny"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Teste em uma Única Céula"
   ],
   "metadata": {
    "id": "0qDWRSj7rApX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Etapa 1: Instalar as bibliotecas necessárias\n",
    "# Descomente a linha abaixo se estiver em um novo ambiente Colab\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer, StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# --- Etapa 2: Carregar o Modelo do Hugging Face Hub ---\n",
    "\n",
    "# O nome do seu repositório no Hugging Face\n",
    "nome_do_seu_modelo_no_hf = \"robsonnicacio/llama-3-8b-amazon-descriptions-tcf3\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = nome_do_seu_modelo_no_hf,\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# Prepara o modelo para uma inferência mais rápida\n",
    "FastLanguageModel.for_inference(model)\n",
    "print(\"Modelo fine-tuned carregado com sucesso do Hugging Face Hub!\")\n",
    "\n",
    "\n",
    "# --- Etapa 3: Configurar a Inferência de Forma Robusta ---\n",
    "\n",
    "# Template de prompt para a inferência\n",
    "LLAMA3_INFERENCE_PROMPT = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Com base no título do produto, gere a sua descrição.\n",
    "Título: {}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Definição dos Critérios de Parada (Stopping Criteria) para uma saída limpa\n",
    "stop_tokens = [\"<|eot_id|>\", \"<|end_of_text|>\"]\n",
    "stop_token_ids = [tokenizer.convert_tokens_to_ids(token) for token in stop_tokens]\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_id in stop_token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
    "\n",
    "\n",
    "# --- Etapa 4: Loop de Teste Interativo ---\n",
    "while True:\n",
    "    titulo_produto = input(\"Digite o título do produto (ou 'sair' para terminar): \")\n",
    "\n",
    "    if titulo_produto.lower() == 'sair':\n",
    "        print(\"Encerrando o teste.\")\n",
    "        break\n",
    "\n",
    "    prompt = LLAMA3_INFERENCE_PROMPT.format(titulo_produto)\n",
    "    inputs = tokenizer([prompt], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    # Usamos o TextStreamer para ver a resposta sendo gerada em tempo real\n",
    "    # skip_special_tokens=True limpa a saída de tokens como <|eot_id|>\n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    _ = model.generate(\n",
    "        **inputs,\n",
    "        streamer = text_streamer,\n",
    "        max_new_tokens = 256,\n",
    "        stopping_criteria = stopping_criteria, # Usa o critério de parada robusto\n",
    "        repetition_penalty = 1.15\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ],
   "metadata": {
    "id": "4ADv3pd6wLen"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyOTP2JCtwYFjtTUTTNJHzEf"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}


