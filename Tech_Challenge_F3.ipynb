{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiTxVzipoBR6"
   },
   "source": [
    "# Preparando os dados para o [fine-tuning]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23647,
     "status": "ok",
     "timestamp": 1758587890518,
     "user": {
      "displayName": "Robson Nicácio",
      "userId": "18051789998754192047"
     },
     "user_tz": 180
    },
    "id": "0Nzpjk36_9hd",
    "outputId": "18a9d65d-545e-4532-f7f8-b54b697f1056"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Montando Google-Drive:\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29799,
     "status": "ok",
     "timestamp": 1756409041819,
     "user": {
      "displayName": "Robson Nicácio",
      "userId": "18051789998754192047"
     },
     "user_tz": 180
    },
    "id": "mVwY5HRdA83o",
    "outputId": "33a3d494-f97c-4ce8-f2c7-35d28d03f5a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M.raw.zip\n",
      "   creating: /content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/\n",
      "  inflating: /content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/lbl.json.gz  \n",
      "  inflating: /content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/trn.json.gz  \n",
      "  inflating: /content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/filter_labels_test.txt  \n",
      "  inflating: /content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/tst.json.gz  \n",
      "  inflating: /content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/filter_labels_train.txt  \n"
     ]
    }
   ],
   "source": [
    "# Descompactando LF-Amazon:\n",
    "\n",
    "#!unzip '/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M.raw.zip' -d '/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JXlp_0pCHQk"
   },
   "outputs": [],
   "source": [
    "# Descompactando arquivos json:\n",
    "\n",
    "#!gunzip '/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/lbl.json.gz'\n",
    "#!gunzip '/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/trn.json.gz'\n",
    "#!gunzip '/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/tst.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 83,
     "status": "ok",
     "timestamp": 1756409910648,
     "user": {
      "displayName": "Robson Nicácio",
      "userId": "18051789998754192047"
     },
     "user_tz": 180
    },
    "id": "i7HZ4ecpH_nI",
    "outputId": "58611242-8353-4161-a7af-44b8c05b0a28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analisando as primeiras 5 linhas de: /content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/trn.json ---\n",
      "Linha 1: {\"uid\": \"0000031909\", \"title\": \"Girls Ballet Tutu Neon Pink\", \"content\": \"High quality 3 layer ballet tutu. 12 inches in length\", \"target_ind\": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 111], \"target_rel\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
      "Linha 2: {\"uid\": \"0000032034\", \"title\": \"Adult Ballet Tutu Yellow\", \"content\": \"\", \"target_ind\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 16, 33, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71], \"target_rel\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
      "Linha 3: {\"uid\": \"0000913154\", \"title\": \"The Way Things Work: An Illustrated Encyclopedia of Technology\", \"content\": \"\", \"target_ind\": [116, 117, 118, 119, 120, 121, 122], \"target_rel\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
      "Linha 4: {\"uid\": \"0001360000\", \"title\": \"Mog's Kittens\", \"content\": \"Judith Kerr&#8217;s best&#8211;selling adventures of that endearing (and exasperating) cat Mog have entertained children for more than 30 years. Now, even infants and toddlers can enjoy meeting this loveable feline. These sturdy little board books&#8212;with their bright, simple pictures, easy text, and hand&#8211;friendly formats&#8212;are just the thing to delight the very young. Ages 6 months&#8211;2 years.\", \"target_ind\": [146, 147, 148, 149, 495], \"target_rel\": [1.0, 1.0, 1.0, 1.0, 1.0]}\n",
      "Linha 5: {\"uid\": \"0001381245\", \"title\": \"Misty of Chincoteague\", \"content\": \"\", \"target_ind\": [151], \"target_rel\": [1.0]}\n"
     ]
    }
   ],
   "source": [
    "# Verificando as cinco primeiras linhas do arquivo [trn.json]:\n",
    "\n",
    "# Defina o caminho para o arquivo de treino que você descompactou\n",
    "caminho_arquivo_treino = r'/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/trn.json'\n",
    "\n",
    "# Vamos ler e imprimir apenas as 5 primeiras linhas\n",
    "print(f\"--- Analisando as primeiras 5 linhas de: {caminho_arquivo_treino} ---\")\n",
    "try:\n",
    "    with open(caminho_arquivo_treino, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            print(f\"Linha {i+1}: {line.strip()}\")\n",
    "            if i >= 4:  # Parar depois de 5 linhas (índice 0 a 4)\n",
    "                break\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao ler o arquivo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1756410507157,
     "user": {
      "displayName": "Robson Nicácio",
      "userId": "18051789998754192047"
     },
     "user_tz": 180
    },
    "id": "TVkBpYpwMFQ6",
    "outputId": "551b3397-8475-418b-bfad-5513d27428f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analisando as primeiras 5 linhas de: /content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/tst.json ---\n",
      "Linha 1: {\"uid\": \"0000032069\", \"title\": \"Adult Ballet Tutu Cheetah Pink\", \"content\": \"\", \"target_ind\": [0, 1, 2, 4, 7, 8], \"target_rel\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
      "Linha 2: {\"uid\": \"0000589012\", \"title\": \"Why Don't They Just Quit? DVD Roundtable Discussion: What Families and Friends need to Know About Addiction and Recovery\", \"content\": \"\", \"target_ind\": [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110], \"target_rel\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
      "Linha 3: {\"uid\": \"0000031852\", \"title\": \"Girls Ballet Tutu Zebra Hot Pink\", \"content\": \"TUtu\", \"target_ind\": [13, 16, 18, 20, 23, 32, 33, 113, 115], \"target_rel\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
      "Linha 4: {\"uid\": \"0000032050\", \"title\": \"Adult Ballet Tutu Purple\", \"content\": \"\", \"target_ind\": [1, 2, 4, 7, 8, 31, 32, 35, 41, 46, 53, 60, 61, 123, 124, 131, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145], \"target_rel\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}\n",
      "Linha 5: {\"uid\": \"0001203088\", \"title\": \"Hilda Boswell's Omnibus - A Treasury of Favorites\", \"content\": \"\", \"target_ind\": [150], \"target_rel\": [1.0]}\n"
     ]
    }
   ],
   "source": [
    "# Verificando as cinco primeiras linhas do arquivo [tst.json]:\n",
    "\n",
    "# Defina o caminho para o arquivo de treino que você descompactou\n",
    "caminho_arquivo_treino = r'/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/tst.json'\n",
    "\n",
    "# Vamos ler e imprimir apenas as 5 primeiras linhas\n",
    "print(f\"--- Analisando as primeiras 5 linhas de: {caminho_arquivo_treino} ---\")\n",
    "try:\n",
    "    with open(caminho_arquivo_treino, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            print(f\"Linha {i+1}: {line.strip()}\")\n",
    "            if i >= 4:  # Parar depois de 5 linhas (índice 0 a 4)\n",
    "                break\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao ler o arquivo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1756410623414,
     "user": {
      "displayName": "Robson Nicácio",
      "userId": "18051789998754192047"
     },
     "user_tz": 180
    },
    "id": "7T0vTL_xOW6H",
    "outputId": "91c93d4a-dc1f-47b8-9f06-324f74c196ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analisando as primeiras 5 linhas de: /content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/lbl.json ---\n",
      "Linha 1: {\"uid\": \"0000032050\", \"title\": \"Adult Ballet Tutu Purple\", \"content\": \"\", \"target_ind\": [], \"target_rel\": []}\n",
      "Linha 2: {\"uid\": \"B00D0DJAEG\", \"title\": \"Adult Ballet Tutu Pastel Rainbow\", \"content\": \"\", \"target_ind\": [], \"target_rel\": []}\n",
      "Linha 3: {\"uid\": \"B00D0F450I\", \"title\": \"Adult Ballet Tutu Black, one size fit most\", \"content\": \"\", \"target_ind\": [], \"target_rel\": []}\n",
      "Linha 4: {\"uid\": \"B00D2JTMS2\", \"title\": \"Adult Tutu Assorted Colors (Hot Pink)\", \"content\": \"\", \"target_ind\": [], \"target_rel\": []}\n",
      "Linha 5: {\"uid\": \"B00D0FDUAY\", \"title\": \"Adult Ballet Tutu Red\", \"content\": \"\", \"target_ind\": [], \"target_rel\": []}\n"
     ]
    }
   ],
   "source": [
    "# Verificando as cinco primeiras linhas do arquivo [lbl.json]:\n",
    "\n",
    "# Defina o caminho para o arquivo de treino que você descompactou\n",
    "caminho_arquivo_treino = r'/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/lbl.json'\n",
    "\n",
    "# Vamos ler e imprimir apenas as 5 primeiras linhas\n",
    "print(f\"--- Analisando as primeiras 5 linhas de: {caminho_arquivo_treino} ---\")\n",
    "try:\n",
    "    with open(caminho_arquivo_treino, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            print(f\"Linha {i+1}: {line.strip()}\")\n",
    "            if i >= 4:  # Parar depois de 5 linhas (índice 0 a 4)\n",
    "                break\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao ler o arquivo: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vpS9EJmQQ2G"
   },
   "source": [
    "# Script de Processamento\n",
    "> Segundo as instruções do Tech Challenge, serão utilizadas somente duas coluas:\n",
    "```\n",
    "...\n",
    "você utilizará as colunas “title” e\n",
    "“content”, que contém título e descrição respectivamente.\n",
    "...\n",
    "(Tech Challenge.pdf)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9121,
     "status": "ok",
     "timestamp": 1758587920832,
     "user": {
      "displayName": "Robson Nicácio",
      "userId": "18051789998754192047"
     },
     "user_tz": 180
    },
    "id": "lBXmK4_cOzSw",
    "outputId": "3a6a6a2a-fd34-4257-b871-cb12587caad6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iniciando o processamento do dataset para o formato nativo Llama-3, com limpeza de HTML...\n",
      "Processados 5000 registros...\n",
      "Processados 10000 registros...\n",
      "Processados 15000 registros...\n",
      "Processados 20000 registros...\n",
      "Processados 25000 registros...\n",
      "Processados 30000 registros...\n",
      "Processados 35000 registros...\n",
      "Processados 40000 registros...\n",
      "Processados 45000 registros...\n",
      "Processados 50000 registros...\n",
      "Limite de 50000 exemplos atingido. Parando o processamento.\n",
      "\n",
      "Processamento concluído! 50000 exemplos válidos foram salvos em '/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/dataset_para_finetuning.jsonl'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import html # Estou usando esta lib para tratar as [Entidades HTML]; quero que o texto seja apresentado corretamente.\n",
    "import re # Importe a biblioteca de expressões regulares para limpeza extra.\n",
    "\n",
    "# --- CONFIGURAÇÃO ---\n",
    "# Caminhos para o Googl-Drive.\n",
    "DRIVE_BASE_PATH = r\"/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M\"\n",
    "INPUT_FILE_PATH = os.path.join(DRIVE_BASE_PATH, 'trn.json')\n",
    "OUTPUT_FILE_PATH = os.path.join(DRIVE_BASE_PATH, 'dataset_para_finetuning.jsonl')\n",
    "\n",
    "# Defina quantos exemplos você quer processar.\n",
    "# Comece com um número menor (ex: 20000) para testar o fluxo.\n",
    "# Para o treino final, você pode aumentar se necessário.\n",
    "MAX_EXAMPLES = 50000\n",
    "count = 0\n",
    "\n",
    "# Template de prompt nativo do Llama-3\n",
    "LLAMA3_PROMPT_TEMPLATE = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Com base no título do produto, gere a sua descrição.\n",
    "Título: {}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|><|end_of_text|>\"\"\"\n",
    "\n",
    "print(\"Iniciando o processamento do dataset para o formato nativo Llama-3, com limpeza de HTML...\")\n",
    "\n",
    "# Usamos 'with' para garantir que os arquivos sejam fechados corretamente\n",
    "with open(INPUT_FILE_PATH, 'r', encoding='utf-8') as infile, \\\n",
    "     open(OUTPUT_FILE_PATH, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "    for line in infile:\n",
    "        # Para o loop quando atingir o número desejado de exemplos\n",
    "        if count >= MAX_EXAMPLES:\n",
    "            print(f\"Limite de {MAX_EXAMPLES} exemplos atingido. Parando o processamento.\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # Carrega a linha atual (que é uma string JSON) para um dicionário Python\n",
    "            original_record = json.loads(line)\n",
    "\n",
    "            # Extrai os campos que nos interessam usando .get() para evitar erros\n",
    "            title = html.unescape(original_record.get('title', ''))\n",
    "            content = html.unescape(original_record.get('content', ''))\n",
    "\n",
    "            # --- Passo de Limpeza e Validação Crucial ---\n",
    "            # Pular registros que não têm título ou que têm conteúdo vazio\n",
    "            if not title or not content:\n",
    "                continue\n",
    "\n",
    "            # --- ETAPA DE LIMPEZA ADICIONAL ---\n",
    "            # Remove a frase \"--This text refers to...\" e qualquer variação dela.\n",
    "            content = re.sub(r'--This text refers to.*', '', content).strip()\n",
    "\n",
    "            # Se após a limpeza o conteúdo ficar vazio, pule o registro.\n",
    "            if not content:\n",
    "                continue\n",
    "            # --- FIM DA LIMPEZA ---\n",
    "\n",
    "            # Cria o novo dicionário no formato de instrução que o modelo espera\n",
    "            # NOTE: A instrução está em português para clareza, enquanto os dados\n",
    "            # de input/output estão em inglês, aproveitando a capacidade\n",
    "            # multilingue do modelo.\n",
    "            # Usamos um único campo \"text\" que já contém o prompt completo e formatado\n",
    "            formatted_text = LLAMA3_PROMPT_TEMPLATE.format(title, content)\n",
    "\n",
    "            # Escrevemos um dicionário contendo apenas a chave \"text\"\n",
    "            new_record = {\"text\": formatted_text}\n",
    "\n",
    "            outfile.write(json.dumps(new_record, ensure_ascii=False) + '\\n')\n",
    "            count += 1\n",
    "\n",
    "            # Imprime um status a cada 1000 registros processados\n",
    "            if count % 5000 == 0:\n",
    "                print(f\"Processados {count} registros...\")\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            # Ignora linhas que não sejam um JSON válido\n",
    "            continue\n",
    "\n",
    "print(f\"\\nProcessamento concluído! {count} exemplos válidos foram salvos em '{OUTPUT_FILE_PATH}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SaKRpGFyOjzt",
    "outputId": "1b2711c8-735b-4bc0-94d6-01d5283a0e5e",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1758587926426,
     "user_tz": 180,
     "elapsed": 68,
     "user": {
      "displayName": "Robson Nicácio",
      "userId": "18051789998754192047"
     }
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\"text\": \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nCom base no título do produto, gere a sua descrição.\\nTítulo: Girls Ballet Tutu Neon Pink<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nHigh quality 3 layer ballet tutu. 12 inches in length<|eot_id|><|end_of_text|>\"}\n",
      "{\"text\": \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nCom base no título do produto, gere a sua descrição.\\nTítulo: Mog's Kittens<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nJudith Kerr’s best–selling adventures of that endearing (and exasperating) cat Mog have entertained children for more than 30 years. Now, even infants and toddlers can enjoy meeting this loveable feline. These sturdy little board books—with their bright, simple pictures, easy text, and hand–friendly formats—are just the thing to delight the very young. Ages 6 months–2 years.<|eot_id|><|end_of_text|>\"}\n",
      "{\"text\": \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nCom base no título do produto, gere a sua descrição.\\nTítulo: Girls Ballet Tutu Neon Blue<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nDance tutu for girls ages 2-8 years. Perfect for dance practice, recitals and performances, costumes or just for fun!<|eot_id|><|end_of_text|>\"}\n",
      "{\"text\": \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nCom base no título do produto, gere a sua descrição.\\nTítulo: The Prophet<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nIn a distant, timeless place, a mysterious prophet walks the sands. At the moment of his departure, he wishes to offer the people gifts but possesses nothing. The people gather round, each asks a question of the heart, and the man's wisdom is his gift. It is Gibran's gift to us, as well, for Gibran's prophet is rivaled in his wisdom only by the founders of the world's great religions. On the most basic topics--marriage, children, friendship, work, pleasure--his words have a power and lucidity that in another era would surely have provoked the description \\\"divinely inspired.\\\" Free of dogma, free of power structures and metaphysics, consider these poetic, moving aphorisms a 20th-century supplement to all sacred traditions--as millions of other readers already have.--Brian Bruya<|eot_id|><|end_of_text|>\"}\n",
      "{\"text\": \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nCom base no título do produto, gere a sua descrição.\\nTítulo: Worship with Don Moen [VHS]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nWorship with Don Moen [VHS]<|eot_id|><|end_of_text|>\"}\n",
      "{\"text\": \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nCom base no título do produto, gere a sua descrição.\\nTítulo: Autumn Story Brambly Hedge<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\\\"!the most research-crammed fantasy ever set before small children!\\\" Sunday Times Magazine<|eot_id|><|end_of_text|>\"}\n",
      "{\"text\": \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nCom base no título do produto, gere a sua descrição.\\nTítulo: Spirit Led-Moving By Grace In The Holy Spirit's Gifts<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nYou can flow effortlessly and powerfully in the gifts of the Holy Spirit today! Join Joseph Prince as he expounds on the nine gifts of the Holy Spirit and be encouraged to know that every believer-including you-can operate in these grace gifts. Understand how the Spirit leads you in using these gifts and why ministering in love is so important. Discover the secret to stepping out in faith and love, and moving super-naturally in the Spirit. Find out also what your gifts are and how to flow in them. Whether it's in the area of your family, ministry or work, the messages in this powerful DVD resource will stir your faith and help you to operate and minister in the power of the Holy Spirit! 6-DVD Set(7 Sermons-approx. total duration: 5hr 47min)<|eot_id|><|end_of_text|>\"}\n",
      "{\"text\": \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nCom base no título do produto, gere a sua descrição.\\nTítulo: The Very Bad Bunny (Beginner Series)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nBy Marilyn Sadler, Illustrated by Roger Bollen<|eot_id|><|end_of_text|>\"}\n",
      "{\"text\": \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nCom base no título do produto, gere a sua descrição.\\nTítulo: Nice for Mice<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nJill Barklem was born in Epping in 1951. After an accident when she was thirteen, Jill was unable to take part in PE or games at school and instead developed her talent for drawing and art. On leaving school, she studied illustration at St Martin's in London. Jill is now a full-time illustrator, working on the series of Brambly Hedge books.<|eot_id|><|end_of_text|>\"}\n",
      "{\"text\": \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nCom base no título do produto, gere a sua descrição.\\nTítulo: The Book of Revelation<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAmerican Baptist pastor, Bible teacher, and writer, Clarence Larkin was born October 28, 1850, in Chester, Delaware County, Pennsylvania. He was converted to Christ at the age of 19 and then felt called to the Gospel ministry, but the doors of opportunity for study and ministry did not open immediately. He then got a job in a bank. When he was 21 years old, he left the bank and went to college, graduating as a mechanical engineer. He continued as a professional draftsman for a while, then he became a teacher of the blind. Later, failing health compelled him to give up his teaching career. After a prolonged rest, he became a manufacturer. When he was converted he had become a member of the Episcopal Church, but in 1882,he became a Baptist and was ordained as a Baptist minister two years later. He went directly from business into the ministry. His first charge was at Kennett Square, Pennsylvania; his second pastorate was at Fox Chase, Pennsylvania, where he remained for 20 years. He was not a premillennialist at the time of his ordination, but his study of the Scriptures, with the help of some books that fell into his hands, led him to adopt the premillennialist position. He began to make large wall charts, which he titled, \\\"\\\"Prophetic Truth,\\\"\\\" for use in the pulpit. These led to his being invited to teach, in connection with his pastoral work, in two Bible institutes. During this time he published a number of prophetical charts, which were widely circulated. When World War I broke out in 1914, he was called on for addresses on The War and Prophecy. Then God laid it on his heart to prepare a work on Dispensational Truth (or God's Plan and Purpose in the Ages), containing a number of charts with descriptive matter. He spent three years of his life designing and drawing the charts and preparing the text. The favorable reception it has had since it was first published in 1918 seems to indicate that the world was waiting for such a book. Because it had a large and wide circulation in this and other lands, the first edition was soon exhausted. It was followed by a second edition, and then, realizing that the book was of permanent value, Larkin revised it and expanded it, printing it in its present form. He went to be with the Lord on January 24, 1924.\\\"<|eot_id|><|end_of_text|>\"}\n"
     ]
    }
   ],
   "source": [
    "# Verificando o conteúdo do arquivo gerado:\n",
    "! head '/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/dataset_para_finetuning.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzBVlUFIoOX5"
   },
   "source": [
    "## Conclusão da preparação dos dados\n",
    "> Foram utilizado 50.000 registros para amostragem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76THYRp_pAOB"
   },
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhAHQVN_dyCY"
   },
   "outputs": [],
   "source": [
    "# Montando Google-Drive:\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kS_CJACpjh5"
   },
   "outputs": [],
   "source": [
    "# Instalando libs necessárias:\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83kXRem0WKJN"
   },
   "outputs": [],
   "source": [
    "# Configuração\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "DATA_PATH = r'/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/LF-Amazon-1.3M/dataset_para_finetuning.jsonl'\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",\n",
    "]\n",
    "\n",
    "# Carrega o dataset a partir do arquivo JSON formatado\n",
    "dataset = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XP63HrhRsO9R"
   },
   "outputs": [],
   "source": [
    "# Instanciando o Modelo:\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = fourbit_models[2],\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QP4UVbhsXqg"
   },
   "outputs": [],
   "source": [
    "# Preparando o Modelo LoRA:\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNHMMMh4tKp5"
   },
   "outputs": [],
   "source": [
    "# Ajustes finais para treinar o modelo\n",
    "\n",
    "# O diretório no seu Drive para salvar o modelo final e os checkpoints\n",
    "output_dir = r\"/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/FineTuning_Outputs\"\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model = model,\n",
    "  tokenizer = tokenizer,\n",
    "  train_dataset = dataset,\n",
    "  dataset_text_field = \"text\",\n",
    "  max_seq_length = max_seq_length,\n",
    "  dataset_num_proc = 2,\n",
    "  packing = False,\n",
    "  args = TrainingArguments(\n",
    "      # --- Parâmetros de Desempenho ---\n",
    "      per_device_train_batch_size = 2,\n",
    "      gradient_accumulation_steps = 4,\n",
    "      warmup_steps = 5,\n",
    "      # max_steps = 60, # Apenas para testes.\n",
    "      num_train_epochs = 2, # Usando todo o dataset duas vezes.\n",
    "      learning_rate = 2e-4, # Força a prestar muito mais atenção aos detalhes de cada exemplo.\n",
    "      fp16 = not is_bfloat16_supported(),\n",
    "      bf16 = is_bfloat16_supported(),\n",
    "      optim = \"adamw_8bit\",\n",
    "      weight_decay = 0.01,\n",
    "      lr_scheduler_type = \"linear\",\n",
    "      seed = 3407,\n",
    "\n",
    "      # --- Parâmetros de LOGGING e CHECKPOINT ---\n",
    "      output_dir = output_dir,          # Diretório para salvar tudo\n",
    "      logging_steps = 1,                 # Mostra a 'loss' a cada passo\n",
    "      report_to = \"none\",              # Desabilita o login do wandb\n",
    "      save_strategy = \"steps\",         # Estratégia para salvar: a cada X passos\n",
    "      save_steps = 100,                 # Salva um checkpoint a cada 100 passos\n",
    "      save_total_limit = 2,            # Mantém apenas os 2 últimos checkpoints para não encher seu Drive\n",
    "  ),\n",
    ")\n",
    "\n",
    "# Para iniciar o treinamento, use:\n",
    "# trainer.train() # <--- Use esta linha se for a PRIMEIRA vez que está treinando\n",
    "# trainer.train(resume_from_checkpoint = True) # <--- Use esta se precisar CONTINUAR um treino interrompido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPVwuciVtODB"
   },
   "outputs": [],
   "source": [
    "# Fazendo o treinamento\n",
    "# E para iniciar o treinamento, use:\n",
    "# trainer.train() # <--- Use esta linha se for a PRIMEIRA vez que está treinando\n",
    "# trainer.train(resume_from_checkpoint = True) # <--- Use esta se precisar CONTINUAR um treino interrompido\n",
    "\n",
    "# trainer_stats = trainer.train()\n",
    "trainer_stats = trainer.train(resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b4y7xG2NctLp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZA_XyU63TZcF"
   },
   "source": [
    "# Testando o Modelo Fine-Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mi5qPuG0QJzM"
   },
   "outputs": [],
   "source": [
    "# Montar o Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Instalar as bibliotecas necessárias.\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_XHgj1Yg3dF"
   },
   "outputs": [],
   "source": [
    "# Carregando o Modelo Fine-Tuned (Método de 2 Etapas)\n",
    "\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# --- Configuração ---\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "# --- CAMINHO PARA O ADAPTADOR TREINADO ---\n",
    "# Este é o caminho para os pesos do fine-tuning\n",
    "caminho_do_adaptador_lora = r\"/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/FineTuning_Outputs/checkpoint-12500\"\n",
    "\n",
    "# --- NOME DO MODELO BASE ORIGINAL ---\n",
    "# Este é o modelo que foi usado para o treinamento inicial\n",
    "nome_do_modelo_base = \"unsloth/llama-3-8b-bnb-4bit\"\n",
    "\n",
    "\n",
    "# ETAPA 1: Carregue o modelo BASE original a partir da internet.\n",
    "# Isso garante que a Unsloth tenha o 'config.json' correto e saiba que está lidando com um Llama 3.\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = nome_do_modelo_base,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "# ETAPA 2: Aplicando o adaptador LoRA treinado (pesos) por cima do modelo base.\n",
    "model.load_adapter(caminho_do_adaptador_lora)\n",
    "\n",
    "\n",
    "# Prepara o modelo final para inferência (mais rápido)\n",
    "FastLanguageModel.for_inference(model)\n",
    "print(\"Modelo carregado com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2yZGlQZoTzT9"
   },
   "outputs": [],
   "source": [
    "# Loop de Teste Interativo (VERSÃO CORRIGIDA PARA LLAMA-3)\n",
    "\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "\n",
    "# Define o template do prompt para INFERÊNCIA.\n",
    "# Note que ele termina exatamente onde a resposta do assistente deve começar.\n",
    "LLAMA3_INFERENCE_PROMPT = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Com base no título do produto, gere a sua descrição.\n",
    "Título: {}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Loop para testar o modelo várias vezes\n",
    "while True:\n",
    "    # Pede um título de produto para o usuário\n",
    "    titulo_produto = input(\"Digite o título do produto (ou 'sair' para terminar): \")\n",
    "\n",
    "    # Condição para sair do loop\n",
    "    if titulo_produto.lower() == 'sair':\n",
    "        print(\"Encerrando o teste.\")\n",
    "        break\n",
    "\n",
    "    # Formata o prompt para o modelo usando o novo template Llama-3\n",
    "    prompt = LLAMA3_INFERENCE_PROMPT.format(titulo_produto)\n",
    "\n",
    "    # Tokeniza o prompt e o envia para a GPU\n",
    "    inputs = tokenizer([prompt], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    # Usa o TextStreamer para ver a resposta sendo gerada em tempo real\n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt=True) # skip_prompt=True é útil aqui\n",
    "    _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n",
    "                       eos_token_id=tokenizer.eos_token_id,\n",
    "                       repetition_penalty=1.15) # Adicionado para parar corretamente\n",
    "\n",
    "    # Adiciona uma linha de separação para o próximo teste\n",
    "    print(\"\\\\n\" + \"=\"*50 + \"\\\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Para Hugging Face Hub"
   ],
   "metadata": {
    "id": "MCOUNUh9MlBK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Montar o Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "5HS17t7IMyJH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Instalar as bibliotecas necessárias.\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ],
   "metadata": {
    "id": "2GfxnHflPjP0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# SALVANDO O MODELO FINAL\n",
    "\n",
    "\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# --- Caminho onde o checkpoint final foi salvo ---\n",
    "caminho_do_checkpoint_final = r\"/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/FineTuning_Outputs/checkpoint-12500\"\n",
    "\n",
    "# --- Onde salvar o modelo pronto para upload ---\n",
    "caminho_para_salvar_modelo = r\"/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/modelo_final_Llama3_8b_TCF3\"\n",
    "\n",
    "# Carrega o modelo a partir do checkpoint final\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = caminho_do_checkpoint_final,\n",
    ")\n",
    "\n",
    "# Salva o modelo e o tokenizador em um novo diretório.\n",
    "# Este comando junta o adaptador LoRA com os arquivos de configuração necessários.\n",
    "model.save_pretrained(caminho_para_salvar_modelo)\n",
    "tokenizer.save_pretrained(caminho_para_salvar_modelo)\n",
    "\n",
    "print(f\"Modelo final salvo com sucesso em: {caminho_para_salvar_modelo}\")"
   ],
   "metadata": {
    "id": "AZnMPlh7NDTC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# UPLOAD PARA O HUGGING FACE HUB\n",
    "\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# token de acesso do Hugging\n",
    "login(\"\")\n",
    "\n",
    "# --- Onde o modelo final foi salvo ---\n",
    "caminho_do_modelo_salvo = r\"/content/drive/MyDrive/Biblioteca/Acadêmico/Pós Graduação/Pós Tech - FIAP/Aulas/Fase_3/Tech_Challenge/Tech_Challenge--5IADT--Fase_03/modelo_final_Llama3_8b_TCF3\"\n",
    "\n",
    "# --- Nome no repositório no Hugging Face ---\n",
    "nome_do_repo_hf = \"robsonnicacio/llama-3-8b-amazon-descriptions-tcf3\"\n",
    "\n",
    "# Carrega o tokenizador e o modelo localmente\n",
    "tokenizer = AutoTokenizer.from_pretrained(caminho_do_modelo_salvo)\n",
    "model = AutoModelForCausalLM.from_pretrained(caminho_do_modelo_salvo)\n",
    "\n",
    "# Faz o upload\n",
    "tokenizer.push_to_hub(nome_do_repo_hf, use_temp_dir=True)\n",
    "model.push_to_hub(nome_do_repo_hf, use_temp_dir=True)\n",
    "\n",
    "print(f\"Upload concluído! Seu modelo está disponível em: https://huggingface.co/{nome_do_repo_hf}\")"
   ],
   "metadata": {
    "id": "1mcuwJ8wNJQq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Usando o Modelo Treinado via Hugging Face Hub"
   ],
   "metadata": {
    "id": "u_y60TZWNfCa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Instalando as bibliotecas necessárias.\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ],
   "metadata": {
    "id": "gX7sDXwPwRkc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Carregando o Modelo Fine-Tuned DIRETAMENTE DO HUGGING FACE\n",
    "# OBS: falta a parte para inferência aqui!\n",
    "\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# --- NOME DO SEU MODELO NO HUGGING FACE ---\n",
    "nome_do_seu_modelo_no_hf = \"robsonnicacio/llama-3-8b-amazon-descriptions-tcf3\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = nome_do_seu_modelo_no_hf,\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# Prepara o modelo final para inferência\n",
    "FastLanguageModel.for_inference(model)\n",
    "print(\"Modelo fine-tuned carregado com sucesso do Hugging Face Hub!\")"
   ],
   "metadata": {
    "id": "iVPbSUJHNqjp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Loop de Teste Interativo (Testando o que está no Hugging Face)\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "\n",
    "# Define o template do prompt para INFERÊNCIA.\n",
    "# Note que ele termina exatamente onde a resposta do assistente deve começar.\n",
    "LLAMA3_INFERENCE_PROMPT = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Com base no título do produto, gere a sua descrição.\n",
    "Título: {}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Loop para testar o modelo várias vezes\n",
    "while True:\n",
    "    # Pede um título de produto para o usuário\n",
    "    titulo_produto = input(\"Digite o título do produto (ou 'sair' para terminar): \")\n",
    "\n",
    "    # Condição para sair do loop\n",
    "    if titulo_produto.lower() == 'sair':\n",
    "        print(\"Encerrando o teste.\")\n",
    "        break\n",
    "\n",
    "    # Formata o prompt para o modelo usando o novo template Llama-3\n",
    "    prompt = LLAMA3_INFERENCE_PROMPT.format(titulo_produto)\n",
    "\n",
    "    # Tokeniza o prompt e o envia para a GPU\n",
    "    inputs = tokenizer([prompt], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    # Usa o TextStreamer para ver a resposta sendo gerada em tempo real\n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt=True) # skip_prompt=True é útil aqui\n",
    "    _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 256,\n",
    "                       eos_token_id=tokenizer.eos_token_id,\n",
    "                       repetition_penalty = 1.15) # Adicionado para parar corretamente\n",
    "\n",
    "    # Adiciona uma linha de separação para o próximo teste\n",
    "    print(\"\\\\n\" + \"=\"*50 + \"\\\\n\")"
   ],
   "metadata": {
    "id": "keua1fHaNwny"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Teste em uma Única Céula"
   ],
   "metadata": {
    "id": "0qDWRSj7rApX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Etapa 1: Instalar as bibliotecas necessárias\n",
    "# Descomente a linha abaixo se estiver em um novo ambiente Colab\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer, StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# --- Etapa 2: Carregar o Modelo do Hugging Face Hub ---\n",
    "\n",
    "# O nome do seu repositório no Hugging Face\n",
    "nome_do_seu_modelo_no_hf = \"robsonnicacio/llama-3-8b-amazon-descriptions-tcf3\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = nome_do_seu_modelo_no_hf,\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# Prepara o modelo para uma inferência mais rápida\n",
    "FastLanguageModel.for_inference(model)\n",
    "print(\"Modelo fine-tuned carregado com sucesso do Hugging Face Hub!\")\n",
    "\n",
    "\n",
    "# --- Etapa 3: Configurar a Inferência de Forma Robusta ---\n",
    "\n",
    "# Template de prompt para a inferência\n",
    "LLAMA3_INFERENCE_PROMPT = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Com base no título do produto, gere a sua descrição.\n",
    "Título: {}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Definição dos Critérios de Parada (Stopping Criteria) para uma saída limpa\n",
    "stop_tokens = [\"<|eot_id|>\", \"<|end_of_text|>\"]\n",
    "stop_token_ids = [tokenizer.convert_tokens_to_ids(token) for token in stop_tokens]\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_id in stop_token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
    "\n",
    "\n",
    "# --- Etapa 4: Loop de Teste Interativo ---\n",
    "while True:\n",
    "    titulo_produto = input(\"Digite o título do produto (ou 'sair' para terminar): \")\n",
    "\n",
    "    if titulo_produto.lower() == 'sair':\n",
    "        print(\"Encerrando o teste.\")\n",
    "        break\n",
    "\n",
    "    prompt = LLAMA3_INFERENCE_PROMPT.format(titulo_produto)\n",
    "    inputs = tokenizer([prompt], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    # Usamos o TextStreamer para ver a resposta sendo gerada em tempo real\n",
    "    # skip_special_tokens=True limpa a saída de tokens como <|eot_id|>\n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    _ = model.generate(\n",
    "        **inputs,\n",
    "        streamer = text_streamer,\n",
    "        max_new_tokens = 256,\n",
    "        stopping_criteria = stopping_criteria, # Usa o critério de parada robusto\n",
    "        repetition_penalty = 1.15\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ],
   "metadata": {
    "id": "4ADv3pd6wLen"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyOTP2JCtwYFjtTUTTNJHzEf"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

